{
    "id": "k80kn82ywfOYKX7ji42O",
    "title": "HARDWARE-FRIENDLY CONVOLUTIONAL NEURAL NETWORK WITH EVEN-NUMBER FILTER SIZE",
    "pdf_url": "https://openreview.net/pdf/k80kn82ywfOYKX7ji42O.pdf",
    "has_revisions": false,
    "authors": [
        "Song Yao",
        "Song Han",
        "Kaiyuan Guo",
        "Jianqiao Wangni",
        "Yu Wang"
    ],
    "created_date": null,
    "original_paper_id": null,
    "invitation": "ICLR.cc/2016/workshop/-/submission",
    "reviews": [
        {
            "date": 1457552302912,
            "review": "title:This paper brings attention to the fact that even-number filter sizes can maximize the efficacy of CNN accelerators.\nrating:6: Marginally above acceptance threshold\nreview:The authors bring attention to the fact that odd-number filter sizes waste computational resources; even-number filter sizes can maximize the efficacy of CNN accelerators. They are able to reduce the complexity of LeNet and VGG11-Nagadomi network with comparable performance in accuracy.Figure 1 is very good to understand what the paper is about. Figure 2, on the other hand, is hard to understand; caption doesn't provide enough information. For Figure 2a, why are there two sizes for each test error and normalized complexity bars? If they are the size of the first and second layer filters, why do  8x8, 4x4 filters have less complexity compared to 4x4, 4x4 filters?In Figure 2b there are two bar sets for 2x2 filters, later in the text it appears that one uses more feature maps, this information should be at least in the caption if not in the chart.Overall, the idea is useful and good to keep in mind.\nconfidence:4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "date": 1456589526138,
            "review": "title:This paper provides an interesting and instructive discussion to the industrial community\nrating:7: Good paper, accept\nreview:In this paper, the authors present a fact that neural networks may become less efficient when odd-sized convolution kernels (like 3x3, 5x5 kernels) are used. The main consideration is from the implementation of the inner-product operation in hardware.Figure 1 is quite intuitive: one can catch the main idea by taking a glance at it.Experimental results are acceptable: with smaller kernels, the recognition performance is comparable while the FLOPs are effectively reduced. It would be better if this idea is verified on some larger experiments such as SVHN and ImageNet.Minor things. (1) The mathematical notations can be more formal: in representing the network structure (20Conv5 ...), please use \\rm{Conv} or \\mathrm{Conv}, also please replace all the 'x' to '\\times' (in 'Conclusion'). (2) Please magnify the font size in both figures, until one can read it clearly on a printed version of the paper. (3) The fonts of digits in Figure 2(a) and Figure 2(b) are different, which is weird.In conclusion, this is a good workshop paper that tells the community a simple yet useful fact.\nconfidence:5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ],
    "decision": null,
    "pdf": "Workshop track - ICLR 2016\n\nHARDWARE-FRIENDLY CONVOLUTIONAL NEURAL\nNETWORK WITH EVEN-NUMBER FILTER SIZE\n\nSong Yao, Kaiyuan Guo, Jianqiao Wangni, Yu Wang\nCenter for Brain-Inspired Computer Research\nDepartment of Electronic Engineering, Tsinghua University\nBeijing 100084, China\n{songyao, yu-wang}@mail.tsinghua.edu.cn,\n{gky15, wnjq11}@mails.tsinghua.edu.cn\n\nSong Han\nDepartment of Electrical Engineering\nStanford University\nStanford, CA 94315, USA\nsonghan@stanford.edu\n\nABSTRACT\n\nConvolutional Neural Network (CNN) has led to great advances in computer vi-\nsion. Various customized CNN accelerators on embedded FPGA or ASIC plat-\nforms have been designed to accelerate CNN and improve energy ef\ufb01ciency. How-\never, the odd-number \ufb01lter size in existing CNN models prevents hardware accel-\nerators from having optimal ef\ufb01ciency. In this paper, we analyze the in\ufb02uences\nof \ufb01lter size on CNN accelerator performance and show that even-number \ufb01l-\nter size is much more hardware-friendly that can ensure high bandwidth and re-\nsource utilization. Experimental results on MNIST and CIFAR-10 demonstrate\nthat hardware-friendly even kernel CNNs can reduce the FLOPs by 1.4\u00d7 to 2\u00d7\nwith comparable accuracy; With same FLOPs, even kernel can have even higher\naccuracy than odd size kernel.\n\n1\n\nINTRODUCTION\n\nIn recent years, Convolutional Neural Network (CNN) has achieved great success in computer vision\narea. State-of-the-art performance of image classi\ufb01cation and object detection are both driven by\nCNN ((He et al., 2015; Girshick et al., 2014; Ren et al., 2015)). However, the energy ef\ufb01ciency of\nexisting hardware such as GPU is relatively low, thus researchers have proposed various customized\nCNN accelerator designs on FPGA or ASIC.\n\nEf\ufb01cient processing engine (PE) is vital to CNN accelerators. Architecture with few complex PEs\n((Qiu et al., 2016; Sim et al., 2016)) or with many simple compute elements (Chen et al. (2016))\nhave been proposed. Special architectures such as a dynamically con\ufb01gurable architecture and a\nspeci\ufb01c architecture for sparse compressed NN were also proposed ((Chakradhar et al., 2010; Han\net al., 2016a)).\n\nThe ef\ufb01ciency of memory system in CNN accelerators also signi\ufb01cantly affects the performance.\nThe tiling strategy and data reuse are useful to reduce the total communication traf\ufb01c (Chen et al.\n(2014a); Qiu et al. (2016)). Storing all the CNN model with on-chip memory can help minimize\nenergy of memory access (Chen et al. (2014b); Du et al. (2015); Han et al. (2016a)). Compression\nand decompression techniques(Zhang et al. (2015); Chen et al. (2016); Han et al. (2015; 2016b)) and\ndata quantization (Qiu et al. (2016)) are also useful techniques to improve bandwidth utilization.\n\nThough techniques have been proposed to improve the performance of customized CNN acceler-\nators, the odd-number \ufb01lter size in existing CNNs still hinders higher hardware acceleration ef-\n\ufb01ciency. From algorithm aspect, the advantage of odd-number \ufb01lter size is obvious: symmetry.\nHowever, customized CNN accelerators may perform better with even-number Conv \ufb01lters such as\n2\u00d72 and 4\u00d74 and can achieve better con\ufb01gurability and resource utilization.\n\nIn this paper, we investigate the effects of Conv \ufb01lter size on hardware acceleration ef\ufb01ciency of\nCNN accelerators. We propose the hardware-friendly CNN with only even-number Conv \ufb01lters to\nmaximize the ef\ufb01cacy of CNN accelerators. We show that hardware-friendly CNNs can achieve\ncomparable or even better accuracy compared with CNN with odd-number Conv \ufb01lters on MNIST\nand CIFAR-10.\n\n1\n\n\fWorkshop track - ICLR 2016\n\nFigure 1: In\ufb02uences of \ufb01lter size on hardware design: Adder tree structure with (a) 3\u00d73 \ufb01lter and (b) 2\u00d72\n\ufb01lter; Memory access pattern with (c) 3\u00d73 \ufb01lter and (d) 2\u00d72 \ufb01lter.\n\n2\n\nINFLUENCES OF FILTER SIZE ON HARDWARE ACCELERATION EFFICIENCY\n\n2.1 COMPUTATION LOGIC DESIGN\nThe combination of many multipliers and an adder tree is a fundamental unit for accelerating Conv\nlayers. For the adder tree, if the number of data in a \ufb01lter is not 2n form, there will be extra register\nused. As shown in Figure 1 (a), the 3 extra register sets are needed to implement an adder tree with\n9 inputs. If 16-bit quantization (i.e. each parameter is represented with 16 bits) is employed, this\nmeans 16 \u00d7 3 = 48 additional registers are needed. For a 2\u00d72 \ufb01lter, as shown in Figure 1 (b), there\nis no such waste of registers.\n\n2.2 DATA DISTRIBUTOR DESIGN\nState-of-the-art CNNs for large-scale object recognition tasks are too large to be store the model\non-chip. Since CNN models are usually stored in the external memory, the bandwidth utilization\nef\ufb01ciency is seriously concerned. Typically, DRAM offers 64-bit or 128-bit data port. If the length\nof the fetched data is folds of the data port width, full bandwidth utilization can be ensured.\n\nIt is hard to ensure high bandwidth utilization with odd-number \ufb01lters. For a 3\u00d73 \ufb01lter with 16-bit\nquantization, 144 bits are needed to store the weights in a \ufb01lter. For a 64-bit port, to load 144 bits,\ntriple memory accesses are needed, as shown in Figure 1 (c), and the highest possible bandwidth\nusage is only 75%. For a 128-bit port, the highest possible bandwidth usage is only 56.25%. To\nfully utilize the bandwidth when the \ufb01lters are in odd-number sizes, the data distributor design will\nbe quite complicated.\n\nEven-number \ufb01lters can help ensure the bandwidth utilization. For a 2N\u00d72N \ufb01lter with 16-bit\nquantization, where N is a natural number, the total number of bits is 64N2. For a 64-bit port, the\nbandwidth utilization is de\ufb01nitely 100%. For a 128-bit port, the bandwidth usage can be up to 100%\n(loading two \ufb01lters at the same time), 90%, and 96% when N is 1, 2, and 3 respectively. An example\nis shown in Figure 1 (d) where the \ufb01lter size is 2\u00d72. When the data port width is 64-bit and 16-bit\nquantization is employed, only one-time memory access is needed to load all the weights.\n\n3 HARDWARE-FRIENDLY CONVOLUTIONAL NEURAL NETWORK\n\nSince CNNs with even-number Conv \ufb01lters can help improve the ef\ufb01ciency of customized CNN\naccelerators, we propose the hardware-friendly CNN with only even-number Conv \ufb01lters. In this\nsection, we evaluate the performance of hardware-friendly CNNs on MNIST (LeCun et al. (1998))\nand CIFAR-10 (Krizhevsky & Hinton (2009)). All experiments are done with MXnet Chen et al.\n(2015). The experiment platform consists of an Intel Xeon E5-2690 CPUs@2.90GHz and the 2\nNVIDIA TITAN X GPUs.\n\nThe notation is: MP means max pooling, FC means Fully-Connected layer, lr is the initial learning\nrate, lr-factor is the factor that times the learning rate for every lr-factor-epoch, and batch-size is\nthe number of images in each mini batch. When training the networks, no data augmentation, pre-\nprocessing, or pre-training is employed.\n\n2\n\n++++++++FFFFFFResultFFFeature mapConvolution kernelOperatorFlip-flops+Adder+++Result(a)(b)(c)(d)12313(cid:104)3 Filter2(cid:104)2 Filter\fWorkshop track - ICLR 2016\n\nFigure 2: Test error and normalized computational complexities (FLOPs) of (a) LeNet5 on MNIST and (b)\nVGG11-Nagadomi on CIFAR-10. With comparable accuracy, even kernel can reduce the FLOP by 50% on\ncifar dataset and 30% on mnist dataset; with comparable FLOPs, even kernel can have higher accuracy than\nodd size kernel. .\n\n3.1 MNIST\n\nFor experiments on MNIST, we used the LeNet (LeCun et al. (1998)). The architecture of the\noriginal LeNet is:\n\n20Conv5 \u2192 T anh \u2192 M P 2 \u2192 50Conv5 \u2192 T anh \u2192 M P 2 \u2192 F C500 \u2192 T anh \u2192 F C10.\nWe train the LeNet for 300 epochs, in which the lr is 0.002, lr-factor is 0.995, lr-factor-epoch is 1,\nand batch-size is 128.\n\nWe report he best validation error rate of LeNet with different settings on MNIST in Figure 2 (a), in\nwhich blue and orange columns represent test errors and computational complexities respectively.\nAs shown in the \ufb01gure, replacing the 5\u00d75 Conv \ufb01lters in LeNet with 4\u00d74 or other even-number\nones does not introduce high error rate. Since smaller Conv \ufb01lter demands few multiplications in\none Conv operation, generally, the total number of operations can be reduce by using smaller even-\nnumber Conv \ufb01lters. But the increase of feature map size lead to the increase of total computations.\n\n3.2 CIFAR-10\n\nWe used the VGG11-Nagadomi network (nag) in experiments on CIFAR-10. The architecture of the\noriginal VGG11-Nagadomi network is:\n\n2 \u00d7 (64Conv3 \u2192 ReLU ) \u2192 M P 2 \u2192 2 \u00d7 (128Conv3 \u2192 ReLU ) \u2192 M P 2 \u2192\n4 \u00d7 (256Conv3 \u2192 ReLU ) \u2192 M P 2 \u2192 2 \u00d7 (F C1024 \u2192 ReLU ) \u2192 F C10.\n\nWe train the VGG11-Nagadomi for 2000 epochs, in which the lr is 0.01, lr-factor is 0.995, lr-factor-\nepoch is 2, and batch-size is 256.\n\nResults of VGG11-Nagadomi network on CIFAR-10 are shown in Figure 2 (b). For the original\nVGG11-Nagadomi network, the validation error on CIFAR-10 is 8.54%. After replacing the 3\u00d73\nConv \ufb01lters with 2\u00d72 ones, the size of feature maps in the network changes. We remove the padding\nin the later Conv layer in each pair of Conv layers to ensure the input feature map of each MP layer\nremains the same. As the middle columns in Figure 2 (b) show, the validation error rises to 8.67%,\nbut the total computations is reduce to 49% of the original network. Since the total computation\nnumber is reduced when simply replacing 3\u00d73 Conv \ufb01lters with 2\u00d72 ones, we increase the \ufb01lter\nnumbers and the out feature vector length of FC layers by 1.5\u00d7 to balance the total operations. In\nthis case, the total computations rise to 1.10\u00d7 compared with the original network but the test error\nis reduced to 7.86%. We notice that, keeping the original ratio between \ufb01lter number in different\nlayers when balancing the total computations may be favorable to achieve the best accuracy.\n\n4 CONCLUSION\n\nIn this paper we propose hardware-friendly convolution neural network using even-sized kernel and\nits advantage over traditional odd-sized kernel. We analyzed the hardware bene\ufb01t of even sized\nkernel w.r.t both arithmetic unit and memory system. Even sized kernel greatly reduces the number\nof computation while maintaining comparable prediction accuracy: on mnist on cifar-10 it reduced\nthe computation by 1.4\u00d7 to 2\u00d7 with less than 0.1% loss of accuracy. On the other hand, shrinking\nthe kernel from 3x3 to 2x2 at the same time of increasing the number of channels, such that the\ntotal number of computation remains the same, will result in better prediction accuracy. This will\nfacilitate building hardware inference engine with higher ef\ufb01ciency.\n\n3\n\n(a)                                  (b)0.90 0.92 0.87 0.99 0.91 1.09 1.00 1.05 0.87 0.73 0.70 0.86 0.000.200.400.600.801.001.205(cid:104)5(cid:712)5(cid:104)54(cid:104)4(cid:712)4(cid:104)48(cid:104)8(cid:712)4(cid:104)44(cid:104)4(cid:712)2(cid:104)28(cid:104)8(cid:712)2(cid:104)22(cid:104)2(cid:712)2(cid:104)2Test Error (%)Normalized Complexity8.54 8.67 7.86 1.00 0.49 1.10 0.002.004.006.008.0010.003\u00d732\u00d722\u00d72Test Error (%)Normalized Complexity\fWorkshop track - ICLR 2016\n\nREFERENCES\nURL https://github.com/nagadomi/kaggle-cifar10-torch7.\n\nSrimat Chakradhar, Murugan Sankaradas, Venkata Jakkula, and Srihari Cadambi. A dynamically con\ufb01gurable\ncoprocessor for convolutional neural networks. In ACM SIGARCH Computer Architecture News, volume 38,\npp. 247\u2013257. ACM, 2010.\n\nTianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang,\nand Zheng Zhang. Mxnet: A \ufb02exible and ef\ufb01cient machine learning library for heterogeneous distributed\nsystems. arXiv preprint arXiv:1512.01274, 2015.\n\nTianshi Chen, Zidong Du, Ninghui Sun, Jia Wang, Chengyong Wu, Yunji Chen, and Olivier Temam. Diannao:\nA small-footprint high-throughput accelerator for ubiquitous machine-learning. In ACM SIGPLAN Notices,\nvolume 49, pp. 269\u2013284. ACM, 2014a.\n\nYu-Hsin Chen, Tushar Krishna, Joel Emer, and Vivienne Sze. Eyeriss: An energy-ef\ufb01cient recon\ufb01gurable\n\naccelerator for deep convolutional neural networks. In ISSCC. IEEE, 2016.\n\nYunji Chen, Tao Luo, Shaoli Liu, Shijin Zhang, Liqiang He, Jia Wang, Ling Li, Tianshi Chen, Zhiwei Xu,\nNinghui Sun, et al. Dadiannao: A machine-learning supercomputer. In MICRO, pp. 609\u2013622. IEEE, 2014b.\n\nZidong Du, Robert Fasthuber, Tianshi Chen, Paolo Ienne, Ling Li, Tao Luo, Xiaobing Feng, Yunji Chen, and\nOlivier Temam. Shidiannao: shifting vision processing closer to the sensor. In Proceedings of the 42nd\nAnnual International Symposium on Computer Architecture, pp. 92\u2013104. ACM, 2015.\n\nRoss Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object\n\ndetection and semantic segmentation. In CVPR, pp. 580\u2013587. IEEE, 2014.\n\nSong Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for ef\ufb01cient\n\nneural networks. arXiv preprint arXiv:1506.02626, 2015.\n\nSong Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A. Horowitz, and William J. Dally. Eie:\nEf\ufb01cient inference engine on compressed deep neural network. arXiv preprint arXiv:1602.01528, 2016a.\n\nSong Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with\n\npruning, trained quantization and huffman coding. ICLR, 2016b.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. arXiv\n\npreprint arXiv:1512.03385, 2015.\n\nAlex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images, 2009.\n\nYann LeCun, L\u00b4eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document\n\nrecognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.\n\nJiantao Qiu, Jie Wang, Song Yao, Kaiyuan Guo, Boxun Li, Erjin Zhou, Jincheng Yu, Tianqi Tang, Ningyi Xu,\nSen Song, Yu Wang, and Huazhong Yang. Going deeper with embedded fpga platform with convolutional\nneural network. In ACM Symposium on Field Programmable Gate Array (FPGA), pp. 1\u201312. ACM, 2016.\n\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with\n\nregion proposal networks. arXiv preprint arXiv:1506.01497, 2015.\n\nJaehyeong Sim, Jun-Seok Park, Minhye Kim, Dongmyung Bae, Yeongjae Choi, and Lee-Sup Kim. A 1.42top-\ns/w deep convolutional neural network recognition processor for intelligent ioe systems. In ISSCC. IEEE,\n2016.\n\nChen Zhang, Peng Li, Guangyu Sun, Yijin Guan, Bingjun Xiao, and Jason Cong. Optimizing fpga-based\naccelerator design for deep convolutional neural networks. In Proceedings of ISFPGA, pp. 161\u2013170. ACM,\n2015.\n\n4\n\n"
}