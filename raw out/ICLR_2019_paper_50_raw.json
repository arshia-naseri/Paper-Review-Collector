{
    "id": "rygZJ2RcF7",
    "original": "HJgdo-69tQ",
    "cdate": 1538087896632,
    "pdate": null,
    "odate": 1538087896657,
    "mdate": null,
    "tcdate": 1538087896632,
    "tmdate": 1683306197758,
    "ddate": null,
    "number": 958,
    "content": {
        "title": "Out-of-Sample Extrapolation with Neuron Editing",
        "abstract": "While neural networks can be trained to map from one specific dataset to another, they usually do not learn a generalized transformation that can extrapolate accurately outside the space of training. For instance, a generative adversarial network (GAN) exclusively trained to transform images of cars from light to dark might not have the same effect on images of horses. This is because neural networks are good at generation within the manifold of the data that they are trained on. However, generating new samples outside of the manifold or extrapolating \"out-of-sample\" is a much harder problem that has been less well studied. To address this, we introduce a technique called neuron editing that learns how neurons encode an edit for a particular transformation in a latent space. We use an autoencoder to decompose the variation within the dataset into activations of different neurons and generate transformed data by defining an editing transformation on those neurons. By performing the transformation in a latent trained space, we encode fairly complex and non-linear transformations to the data with much simpler distribution shifts to the neuron's activations. We showcase our technique on image domain/style transfer and two biological applications: removal of batch artifacts representing unwanted noise and modeling the effect of drug treatments to predict synergy between drugs.",
        "keywords": [
            "generative adversarial networks",
            "computational biology",
            "generating",
            "generation",
            "extrapolation",
            "out-of-sample",
            "neural network inference"
        ],
        "authorids": [
            "matthew.amodio@yale.edu",
            "davidvandijk@gmail.com",
            "ruth.montgomery@yale.edu",
            "guy.wolf@yale.edu",
            "smita.krishnaswamy@yale.edu"
        ],
        "authors": [
            "Matthew Amodio",
            "David van Dijk",
            "Ruth Montgomery",
            "Guy Wolf",
            "Smita Krishnaswamy"
        ],
        "TL;DR": "We reframe the generation problem as one of editing existing points, and as a result extrapolate better than traditional GANs.",
        "pdf": "/pdf/ee973975dafaef863742edcf43043414dbeb15f7.pdf",
        "paperhash": "amodio|outofsample_extrapolation_with_neuron_editing",
        "_bibtex": "@misc{\namodio2019outofsample,\ntitle={Out-of-Sample Extrapolation with Neuron Editing},\nauthor={Matthew Amodio and David van Dijk and Ruth Montgomery and Guy Wolf and Smita Krishnaswamy},\nyear={2019},\nurl={https://openreview.net/forum?id=rygZJ2RcF7},\n}"
    },
    "forum": "rygZJ2RcF7",
    "referent": null,
    "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission",
    "replyto": null,
    "readers": [
        "everyone"
    ],
    "nonreaders": [],
    "signatures": [
        "ICLR.cc/2019/Conference"
    ],
    "writers": [
        "ICLR.cc/2019/Conference"
    ],
    "details": {
        "revisions": true,
        "directReplies": [
            {
                "id": "ryeR_EJ937",
                "original": null,
                "number": 1,
                "cdate": 1541170294099,
                "ddate": null,
                "tcdate": 1541170294099,
                "tmdate": 1541533546020,
                "tddate": null,
                "forum": "rygZJ2RcF7",
                "replyto": "rygZJ2RcF7",
                "invitation": "ICLR.cc/2019/Conference/-/Paper958/Official_Review",
                "content": {
                    "title": "Interesting idea in improving data transformation generalization across input data distributions in an unsupervised way",
                    "review": "The authors proposed a novel method of making data transformation that is much easier to extend to the cases where the input distribution is different from the one that is used to the train the model (in-sample vs out-sample). This has a lot of application in removing experimental noise in biological data (also known as batch effects).\nThe idea is to learn a representation that separates background (dimensions that do NOT vary across data points, but may be subject to change in a data transformation) and foreground (dimensions that vary between data points under the same background) and then apply a *fixed* linear transformation in the learned representation space. This is different from other approaches, such as GAN, where the transformation is learned entirely based on the data. In addition, it mitigates some known problems, such as the \"mode collapse\" in GAN, by just learning a good representation. This is proposed to be done by an autoencoder trained on both in-samples and out-samples (the transformation is however adjusted based on the in-samples only). Experimental results are appealing in different applications compared to GAN, ResnetGAN, and CycleGAN. \nHere are my major concerns:\n- The idea seems to be very general and indeed is applicable to any latent representation learning method, and not just autoencoders. Is there any reason that other more complicated unsupervised representation learning methods were not used for benchmarking in the paper?\n\n- The method heavily relies on the quality of the unsupervised learned representation. How one is guaranteed that the transformation in the learned space be simple and piecewise linear? Shouldn't we consider a regularization method to guide the unsupervised learning more appropriately? \n\n- The method also implicitly assumes that the same neurons model background and foreground in the in-sample and out-sample data points. How is that guaranteed in practice?",
                    "rating": "6: Marginally above acceptance threshold",
                    "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
                },
                "signatures": [
                    "ICLR.cc/2019/Conference/Paper958/AnonReviewer3"
                ],
                "readers": [
                    "everyone"
                ],
                "nonreaders": [],
                "writers": [
                    "ICLR.cc/2019/Conference"
                ]
            },
            {
                "id": "SklGFZbphQ",
                "original": null,
                "number": 2,
                "cdate": 1541374329736,
                "ddate": null,
                "tcdate": 1541374329736,
                "tmdate": 1544674793299,
                "tddate": null,
                "forum": "rygZJ2RcF7",
                "replyto": "rygZJ2RcF7",
                "invitation": "ICLR.cc/2019/Conference/-/Paper958/Official_Review",
                "content": {
                    "title": "border-line paper",
                    "review": "The paper demonstrates that we can harness (semantically meaningful) features learned by a pre-trained autoencoder AE to define a determinisc transformation (e.g. math operations on latent space) to transform one distribution A into another distribution B.\nThe original AE was pre-trained on a larger distribution that includes both A and B.\n\nA key contribution of this paper is the interesting demonstration that this method (called Neuron Editing) allows us to perform a transformation T that transforms  pre-treatment observations into post-treatment observations, which is useful in the medical or biological setting.\n\n+ Novelty\nNeuron editing is essentially a common technique of performing arithmetics in the latent space e.g. King - Man + Woman = Queen (in NLP) or Man wearing sunglass - Man + Woman = Woman wearing sunglasses (e.g. in image domain e.g. in Alec Radford et al. 2015).\nTherefore, the novelty is limited.\n\n+ Significance\nThe main contribution of this paper is the empirical demonstration that such transformation T is better defined, rather than learned directly from data (e.g. via GANs).\n\nI should note that I'm not too familiar with the biology datasets in Sec. 3.2 and Sec 3.3 in order to fully appreciate the practical impact of Neuron Editing.\n\n+ Clarity\n\nI think some key reasons behind why Neuron Editing works could be more clearly presented.\nThat is, the key here is we use pre-trained AEs to perform a pre-defined transformation.\nI think the key might not be whether we use GANs or not, it is how we use them.\nI guess if we use ALI (i.e. training a GAN concurrently with an AE) to perform Neuron Editing, the result should work as well.",
                    "rating": "5: Marginally below acceptance threshold",
                    "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
                },
                "signatures": [
                    "ICLR.cc/2019/Conference/Paper958/AnonReviewer1"
                ],
                "readers": [
                    "everyone"
                ],
                "nonreaders": [],
                "writers": [
                    "ICLR.cc/2019/Conference"
                ]
            },
            {
                "id": "H1xmTQtGpX",
                "original": null,
                "number": 3,
                "cdate": 1541735354714,
                "ddate": null,
                "tcdate": 1541735354714,
                "tmdate": 1544667985677,
                "tddate": null,
                "forum": "rygZJ2RcF7",
                "replyto": "rygZJ2RcF7",
                "invitation": "ICLR.cc/2019/Conference/-/Paper958/Official_Review",
                "content": {
                    "title": "Novel but Limited on Image Data",
                    "review": "The authors present a way to transform data from a source distribution to have characteristics of a target distribution.\nThis is accomplished by applying a \"NeuronEdit\" function to the encoding of the input; this edited input is then decoded.\nThe NeuronEdit function is parametrized by the target distribution's statistics. The edit function does a sort of simple histogram matching, so that the ith percentile values of the source distribution's bottleneck representations instead become the ith percentile values of the target distribution's bottleneck representations.\nExperiments are on CIFAR-10 and biology datasets (the latter of which are not my strong suit).\n\nThis paper is well-written and original. It is original because there are only a few works which directly manipulate the latent space (one example is latent space interpolation used to visualize GANs), and this is distinct from those.\nThe problem they aim to solve also has not received much attention, which enhances the novelty of this paper.\nThe presented method is simple and easy to implement, since the editing function is not learned but is instead deterministic. It is encapsulated in Equation 1.\n\nThe fact that the editing function is fixed may greatly hinder its flexibility and applicability.\nIn Section 3.1 and Figure 1, we are shown that NeuronEditting can turn images of horses with white backgrounds into images of horses with dark backgrounds (horses are an unseen class).\nNeuronEditting turns the horse darker as well. It seems that one could change the brightness and contrast of the image to obtain a similar effect, or one could or take the geometric mean of the image in [0,1] with the average target image and obtain a similar effect. Such traditional methods are also robust to unseen classes. Moreover, NeuronEditting's ability to change the brightness of the image is not that surprising given that brightness is some of the most basic image information. (In point of fact it is captured by the DC Coefficient, the very first coefficient from the discrete cosine transform which is used in JPEG.) What else can NeuronEditting do in the image domain? Can this be used to rotate or reflect MNIST digits? The biological experiments also appear to involve simple input transformations.\n\nFine points:\n- \"an edit function between the the\"\n- I am not sure the speculation about this method's loose relation to word2vec belongs in a scientific work. Both involve modifications to a neural representations, but no further relation is justified in the paper.\n- Was the dataset partitioning for the CIFAR-10 experiment done manually? If not, what process partitioned the dataset?\n\nEdit: Some of my suggestions were incorporated in the rebuttal, but my sentiment is still that this is almost at the acceptance threshold. The large focus on biology makes much of this paper harder to evaluate or appreciate.",
                    "rating": "5: Marginally below acceptance threshold",
                    "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
                },
                "signatures": [
                    "ICLR.cc/2019/Conference/Paper958/AnonReviewer2"
                ],
                "readers": [
                    "everyone"
                ],
                "nonreaders": [],
                "writers": [
                    "ICLR.cc/2019/Conference"
                ]
            },
            {
                "id": "ByebsVQVlV",
                "original": null,
                "number": 1,
                "cdate": 1544987800967,
                "ddate": null,
                "tcdate": 1544987800967,
                "tmdate": 1545354488264,
                "tddate": null,
                "forum": "rygZJ2RcF7",
                "replyto": "rygZJ2RcF7",
                "invitation": "ICLR.cc/2019/Conference/-/Paper958/Meta_Review",
                "content": {
                    "metareview": "This was a borderline paper, as reviewers generally agreed that the method was a new method that was appropriately explained and motivated and had reasonable experimental results. The main drawbacks were that the significance of the method was unclear. In particular, the method might be too inflexible due to being based on a hard-coded rule, and it is not clear why this is the right approach relative to e.g. GANs with a modified training objective). Reviewers also had difficulty assessing the significance of the results on biological datasets. While such results certainly add to the paper, the paper would be stronger if the argument for significance could be assessed from more standard datasets.\n\nA note on the review process: the reviewers initially scored the paper 6/6/6, but the review text for some of the reviews was more negative than a typical 6 score. To confirm this, I asked if any reviewers wanted to push for acceptance. None of the reviewers did (generally due to feeling the significance of the results was limited) and two of the reviewers decided to lower their scores to account for this.",
                    "confidence": "2: The area chair is not sure",
                    "recommendation": "Reject",
                    "title": "reasonable clarity and quality but unclear significance"
                },
                "signatures": [
                    "ICLR.cc/2019/Conference/Paper958/Area_Chair1"
                ],
                "readers": [
                    "everyone"
                ],
                "nonreaders": [],
                "writers": [
                    "ICLR.cc/2019/Conference/Paper958/Area_Chair1"
                ]
            }
        ],
        "invitation": {
            "reply": {
                "readers": {
                    "values": [
                        "everyone"
                    ]
                },
                "writers": {
                    "values": [
                        "ICLR.cc/2019/Conference"
                    ]
                },
                "signatures": {
                    "values": [
                        "ICLR.cc/2019/Conference"
                    ]
                },
                "content": {
                    "title": {
                        "value-regex": ".{1,250}",
                        "required": true,
                        "description": "Title of paper.",
                        "order": 1
                    },
                    "abstract": {
                        "value-regex": "[\\S\\s]{1,5000}",
                        "required": true,
                        "description": "Abstract of paper.",
                        "order": 8
                    },
                    "keywords": {
                        "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*",
                        "description": "Comma separated list of keywords describing this paper",
                        "order": 6
                    },
                    "authorids": {
                        "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})",
                        "required": true,
                        "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized.",
                        "order": 3
                    },
                    "authors": {
                        "values": [
                            "Anonymous"
                        ],
                        "required": true,
                        "description": "Comma separated list of author names. Please provide real names; identities will be anonymized.",
                        "order": 2
                    },
                    "TL;DR": {
                        "value-regex": "[^\\n]{0,250}",
                        "required": false,
                        "description": "\"Too Long; Didn't Read\": a short sentence describing your paper",
                        "order": 7
                    },
                    "pdf": {
                        "value-regex": "upload",
                        "required": true,
                        "description": "Upload a PDF file that ends with .pdf",
                        "order": 9
                    },
                    "code": {
                        "required": false,
                        "description": "Optional link to data sources",
                        "value-regex": ".*",
                        "markdown": true,
                        "order": 100
                    },
                    "data": {
                        "required": false,
                        "description": "Optional link to data sources",
                        "value-regex": ".*",
                        "markdown": true,
                        "order": 101
                    },
                    "community_implementations": {
                        "required": false,
                        "description": "Optional link to open source implementations",
                        "value-regex": ".*",
                        "markdown": true
                    }
                }
            },
            "cdate": 1538142958393,
            "duedate": 1538085600000,
            "signatures": [
                "ICLR.cc/2019/Conference"
            ],
            "readers": [
                "everyone"
            ],
            "nonreaders": [],
            "writers": [
                "ICLR.cc/2019/Conference"
            ],
            "invitees": [
                "~"
            ],
            "noninvitees": [],
            "tmdate": 1679773905079,
            "id": "ICLR.cc/2019/Conference/-/Blind_Submission",
            "type": "note"
        }
    }
}