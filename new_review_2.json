{
    "id": "H1hlWndxM",
    "original": null,
    "cdate": 1511731443944,
    "pdate": null,
    "odate": null,
    "mdate": null,
    "tcdate": 1511731443944,
    "tmdate": 1515761722048,
    "ddate": null,
    "number": 2,
    "content": {
        "title": "Interesting nonlinear ICA method, but unfocused presentation and poor comparisons",
        "rating": "5: Marginally below acceptance threshold",
        "review": "The paper proposes a GAN variant for solving the nonlinear independent component analysis (ICA) problem. The method seems interesting, but the presentation has a severe lack of focus.\n\nFirst, the authors should focus their discussion instead of trying to address a broad range of ICA problems from linear to post-nonlinear (PNL) to nonlinear. I would highly recommend the authors to study the review \"Advances in Nonlinear Blind Source Separation\" by Jutten and Karhunen (2003/2004) to understand the problems they are trying to solve.\n\nLinear ICA is a solved problem and the authors do not seem to be able to add anything there, so I would recommend dropping that to save space for the more interesting material.\n\nPNL ICA is solvable and there are a number of algorithms proposed for it, some cited already in the above review, but also more recent ones. From this perspective, the presented comparison seems quite inadequate.\n\nFully general nonlinear ICA is ill-posed, as shown already by Darmois (1953, doi:10.2307/1401511). Given this, the authors should indicate more clearly what is their method expected to do. There are an infinite number of nonlinear ICA solutions - which one is the proposed method going to return and why is that relevant? There are fewer relevant comparisons here, but at least Lappalainen and Honkela (2000) seem to target the same problem as the proposed method.\n\nThe use of 6 dimensional example in the experiments is a very good start, as higher dimensions are quite different and much more interesting than very commonly used 2D examples.\n\nOne idea for evaluation: comparison with ground truth makes sense for PNL, but not so much for general nonlinear because of unidentifiability. For general nonlinear ICA you could consider evaluating the quality of the estimated low-dimensional data manifold or evaluating the mutual information of separated sources on new test data.\n\nUpdate after author feedback: thanks for the response and the revision. The revision seems more cosmetic and does not address the most significant issues so I do not see a need to change my evaluation.",
        "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
    },
    "forum": "ryykVe-0W",
    "referent": null,
    "invitation": "ICLR.cc/2018/Conference/-/Paper573/Official_Review",
    "replyto": "ryykVe-0W",
    "readers": [
        "everyone"
    ],
    "nonreaders": [],
    "signatures": [
        "ICLR.cc/2018/Conference/Paper573/AnonReviewer1"
    ],
    "writers": [],
    "Sub(s)": [
        {
            "id": "Sk1uKXTXf",
            "original": null,
            "cdate": 1515170150774,
            "pdate": null,
            "odate": null,
            "mdate": null,
            "tcdate": 1515170150774,
            "tmdate": 1515170150774,
            "ddate": null,
            "number": 2,
            "content": {
                "title": "Thanks for the feedback.",
                "comment": "We first like to thank the reviewer for the valuable feedback and suggestions.\n\nWe acknowledge that the linear and PNL ICA problems are more or less solved. However, we respectfully disagree that we should drop the treatment of these problems because we still think it is interesting that they can be solved with a new approach which in our opinion is very different from previous methods. This was not obvious to us when we started our research. \n\nA better definition of the version of the non-linear problem would indeed have been desirable in the context of source separation. While we presented the overcomplete case as a first step for evaluating the method, we surely realize that it doesn't come with any theoretical guarantees and that the obtained correlation scores are limited in interpretability. We adjusted the text to make this more clear. \n\nThe alternative to use estimates of the mutual information is certainly something we considered for both evaluation and model selection but this proved to be difficult in general. We tried both Kraskov's nearest-neighbor estimator and the Hilbert Schmidt Independence Criterion but both these estimators typically seemed to consider the features fully independent during most stages of training and don't take into account how informative they are about the input. We still like to thank the reviewer for motivating us to pursue this direction further and like to hear more in detail what is meant with the \"quality of the low-dimensional data manifold\". If there is some principled way of measuring the latter we would certainly like to investigate it.\n\nThanks.\n\n"
            },
            "forum": "ryykVe-0W",
            "referent": null,
            "invitation": "ICLR.cc/2018/Conference/-/Paper573/Official_Comment",
            "replyto": "H1hlWndxM",
            "readers": [
                "everyone"
            ],
            "nonreaders": [],
            "signatures": [
                "ICLR.cc/2018/Conference/Paper573/Authors"
            ],
            "writers": [
                "ICLR.cc/2018/Conference/Paper573/Authors"
            ],
            "Sub(s)": []
        }
    ]
}